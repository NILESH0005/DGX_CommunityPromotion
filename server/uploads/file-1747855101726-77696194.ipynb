{"cells":[{"cell_type":"markdown","id":"ac6663dc","metadata":{"id":"ac6663dc"},"source":["# **Tested and Modified for NVIDIA Infrastructure**"]},{"cell_type":"markdown","id":"6fa96a4d","metadata":{"id":"6fa96a4d"},"source":["## **BLIP (Bootstrapping Language-Image Pre-training)**\n","\n","BLIP is a state-of-the-art model designed for vision-language tasks such as image captioning, image-text retrieval, and visual question answering. It leverages large-scale pretraining to learn rich representations that jointly understand both images and natural language. BLIP was introduced to bridge the gap between visual understanding and natural language generation in a scalable and efficient manner.\n","\n","\n","BLIP uses a combination of vision encoders and language models, pretrained on massive amounts of image-text pairs. The model bootstraps itself through an iterative training process, improving its multimodal understanding by aligning image features and text embeddings.\n","\n","![IMDB](https://www.researchgate.net/profile/Zeyu-Xiong-5/publication/367369926/figure/fig2/AS:11431281114845784@1674699948232/A-working-example-of-BLIP-Image-Captioning.ppm)\n","\n","\n","**Here's how BLIP works:**\n","\n","1. Vision and Language Encoders:\n","BLIP consists of a vision encoder  that processes images into visual embeddings, and a language encoder (usually a Transformer-based model like BERT) that processes text inputs.\n","\n","2. Bootstrapping Pretraining:\n","BLIP employs a novel bootstrapping approach where it alternates between generating captions for images and improving its understanding of the alignment between image and text. It uses large datasets of image-text pairs from the web for this pretraining.\n","\n","3. Contrastive Learning:\n","BLIP aligns image and text embeddings in a shared semantic space using contrastive loss. This encourages the model to bring matching image-text pairs closer while pushing apart non-matching pairs, improving cross-modal retrieval.\n","\n","4. Generative Fine-tuning:\n","Beyond retrieval, BLIP can be fine-tuned to generate descriptive captions for images. This generative capability is achieved using a sequence-to-sequence Transformer that conditions language generation on visual features.\n","\n","5. Applications:\n","BLIP is versatile and achieves strong performance on tasks like image captioning, image-text retrieval, and visual question answering, enabling machines to better understand and describe visual content in natural language.\n","\n","**References:**\n","\n","1. https://ahmed-sabir.medium.com/paper-summary-blip-bootstrapping-language-image-pre-training-for-unified-vision-language-c1df6f6c9166\n","2. https://www.researchgate.net/figure/A-working-example-of-BLIP-Image-Captioning_fig2_367369926"]},{"cell_type":"markdown","id":"114da022","metadata":{"id":"114da022"},"source":["## **Install required dependencies**\n","\n","Below command installs four popular Python libraries essential for deep learning, especially in natural language processing (NLP) and computer vision tasks.\n","\n","- **Transformers** use and fine-tune pretrained language and multimodal models. In our case, it provides the BlipProcessor and BlipForConditionalGeneration needed for caption generation.\n","\n","- **Torchvision** access computer vision datasets and prebuilt models.\n","\n","- **Pillow** load, process, and save images easily."]},{"cell_type":"code","execution_count":null,"id":"5c7208e9","metadata":{"id":"5c7208e9"},"outputs":[],"source":["pip install transformers torchvision Pillow"]},{"cell_type":"markdown","id":"778876dc","metadata":{"id":"778876dc"},"source":["## **Import Required Dependencies**\n","\n","**PIL.Image:** The Python Imaging Library (Pillow) is used to open and process images in the required format.\n","\n","**BlipProcessor** handles all the necessary preprocessing required before passing an image to the BLIP model. This includes resizing, normalization, and formatting the image into tensors compatible with the model.\n","\n","**BlipForConditionalGeneration** is the core pre-trained BLIP model from Hugging Face's Transformers library. It performs the actual image caption generation based on the processed image input.\n","The from_pretrained() method loads the model weights that were fine-tuned specifically for image captioning tasks."]},{"cell_type":"code","execution_count":null,"id":"80497881","metadata":{"id":"80497881"},"outputs":[],"source":["import torch\n","from transformers import BlipProcessor, BlipForConditionalGeneration,  Trainer, TrainingArguments\n","from PIL import ExifTags, Image"]},{"cell_type":"code","execution_count":null,"id":"082c00dc","metadata":{"id":"082c00dc"},"outputs":[],"source":["pip uninstall pillow -y"]},{"cell_type":"code","execution_count":null,"id":"42d21f94","metadata":{"id":"42d21f94"},"outputs":[],"source":["pip install pillow==8.2.0"]},{"cell_type":"markdown","id":"426bcf85","metadata":{"id":"426bcf85"},"source":["## **Download and load the pre-trained BLIP model and processor**"]},{"cell_type":"code","execution_count":null,"id":"77cf86aa","metadata":{"id":"77cf86aa"},"outputs":[],"source":["processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")"]},{"cell_type":"code","execution_count":null,"id":"f56f8143","metadata":{"id":"f56f8143"},"outputs":[],"source":["model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"]},{"cell_type":"markdown","id":"b2ed5e53","metadata":{"id":"b2ed5e53"},"source":["## **Load an image**"]},{"cell_type":"code","execution_count":null,"id":"2c840686","metadata":{"id":"2c840686"},"outputs":[],"source":["image_path = \"rain1.jpg\"  # Replace with your image path\n","image = Image.open(image_path).convert(\"RGB\")"]},{"cell_type":"markdown","id":"3014faf1","metadata":{"id":"3014faf1"},"source":["## **Preprocess the image and prepare the input**\n"]},{"cell_type":"code","execution_count":null,"id":"9257631a","metadata":{"id":"9257631a"},"outputs":[],"source":["inputs = processor(images=image, return_tensors=\"pt\")"]},{"cell_type":"markdown","id":"b8aba8ec","metadata":{"id":"b8aba8ec"},"source":["## **Generate captions using BLIP**"]},{"cell_type":"code","execution_count":null,"id":"b2f23e2e","metadata":{"id":"b2f23e2e"},"outputs":[],"source":["out = model.generate(**inputs)\n","caption = processor.decode(out[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"id":"68eafc92","metadata":{"id":"68eafc92"},"outputs":[],"source":["print(\"Generated Caption: \", caption)"]},{"cell_type":"markdown","id":"c10092cf","metadata":{"id":"c10092cf"},"source":["## **Fine tuning on small custom dataset**\n","\n","Fine tuning in machine learning is the process of taking a pre-trained model and further training it on a new, smaller dataset relevant to a specific task. This process helps the model to adapt its parameters and improve its performance on that new task by building upon its existing knowledge."]},{"cell_type":"markdown","id":"a324ba6f","metadata":{"id":"a324ba6f"},"source":["**creating small dummy dataset** (you can expand it with more images and captions or add datset from hugging face)"]},{"cell_type":"code","execution_count":null,"id":"79bc2b41","metadata":{"id":"79bc2b41"},"outputs":[],"source":["data = {\n","    \"image\": [\n","        Image.open(\"dog1.jpg\").convert(\"RGB\"), # replace with your image\n","        Image.open(\"ppl1.jpg\").convert(\"RGB\") # replace with your image\n","    ],\n","    \"text\": [\n","        \"A dog playing in the grass.\", # add caption as per image\n","        \"A group of people hiking a mountain.\" # add caption as per image\n","    ]\n","}"]},{"cell_type":"code","execution_count":null,"id":"f1f70807","metadata":{"id":"f1f70807"},"outputs":[],"source":["from datasets import Dataset"]},{"cell_type":"markdown","id":"185a345e","metadata":{"id":"185a345e"},"source":["**This library is used for handling large datasets efficiently and is tightly integrated with the transformers library for NLP and multimodal tasks.**"]},{"cell_type":"code","execution_count":null,"id":"246fb831","metadata":{"id":"246fb831"},"outputs":[],"source":["# Convert to Hugging Face Dataset\n","dataset = Dataset.from_dict(data)\n"]},{"cell_type":"code","execution_count":null,"id":"4306a284","metadata":{"id":"4306a284"},"outputs":[],"source":["# Extract image and caption from the dataset example\n","def preprocess_function(example):\n","    image = example[\"image\"]\n","    caption = example[\"text\"]\n","\n","    # Tokenize and encode the image + caption\n","    encoding = processor(images=image, text=caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n","\n","   # Return a dictionary with processed inputs\n","    return {\n","        \"pixel_values\": encoding[\"pixel_values\"].squeeze(0),\n","        \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n","        \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n","        \"labels\": encoding[\"input_ids\"].squeeze(0),\n","    }\n"]},{"cell_type":"markdown","id":"c8f0f50f","metadata":{"id":"c8f0f50f"},"source":["### **Applying function to each example in the dataset**\n","   Remove original 'image' and 'text' fields after processing"]},{"cell_type":"code","execution_count":null,"id":"9713847d","metadata":{"id":"9713847d"},"outputs":[],"source":["processed_dataset = dataset.map(\n","    preprocess_function,\n","    remove_columns=[\"image\", \"text\"]\n",")\n"]},{"cell_type":"markdown","id":"e2ed88d3","metadata":{"id":"e2ed88d3"},"source":["## **Setting up training hyperparameters with training arguments**\n","The TrainingArguments class defines key settings for model training, such as output directory, batch size, number of epochs, logging frequency, checkpoint saving, and whether to use mixed-precision (FP16) if a GPU is available."]},{"cell_type":"code","execution_count":null,"id":"0ddaadad","metadata":{"id":"0ddaadad"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"./blip-finetuned\",\n","    per_device_train_batch_size=2,\n","    num_train_epochs=5,\n","    logging_steps=1,\n","    save_steps=5,\n","    save_total_limit=1,\n","    remove_unused_columns=False,\n","    fp16=torch.cuda.is_available(),\n","    report_to=\"none\"\n",")"]},{"cell_type":"markdown","id":"41657971","metadata":{"id":"41657971"},"source":["## **Initializing the Trainer for Model Training**\n","The Trainer class handles the full training loop by combining the model, training arguments, dataset, and tokenizer. It simplifies the training process by managing batching, optimization, logging, and checkpointing automatically."]},{"cell_type":"code","execution_count":null,"id":"227c70d9","metadata":{"id":"227c70d9"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=processed_dataset,\n","    tokenizer=processor,\n",")"]},{"cell_type":"markdown","id":"9b8e5b56","metadata":{"id":"9b8e5b56"},"source":["## **Starting the Training Process**\n","The trainer.train() command begins the model training using the configured settings, dataset, and model. It runs the training loop, updating model weights over the specified epochs."]},{"cell_type":"code","execution_count":null,"id":"c6f37e36","metadata":{"id":"c6f37e36"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","id":"ad0de8b3","metadata":{"id":"ad0de8b3"},"source":["## **Saving the Trained Model and Processor**\n","save the fine-tuned model and the associated processor/tokenizer to the specified directory for later use or deployment.\n","\n","The fine tune model is saved inside the blip-finetuned directory"]},{"cell_type":"code","execution_count":null,"id":"93a91a97","metadata":{"id":"93a91a97"},"outputs":[],"source":["trainer.save_model(\"./blip-finetuned\")\n","processor.save_pretrained(\"./blip-finetuned\")"]},{"cell_type":"markdown","id":"79a02081","metadata":{"id":"79a02081"},"source":["## **Load the pre-train model from blip-finetuned directory**\n","\n","Testing on fine tune model"]},{"cell_type":"code","execution_count":null,"id":"d9626253","metadata":{"id":"d9626253"},"outputs":[],"source":["model_fine_tune = BlipForConditionalGeneration.from_pretrained(\"./blip-finetuned\")\n","processor_fine_tune = BlipProcessor.from_pretrained(\"./blip-finetuned\")"]},{"cell_type":"code","execution_count":null,"id":"1e788c3d","metadata":{"id":"1e788c3d"},"outputs":[],"source":["image = Image.open(\"rain1.jpg\").convert(\"RGB\") # replace with your image"]},{"cell_type":"code","execution_count":null,"id":"aee3cd16","metadata":{"id":"aee3cd16"},"outputs":[],"source":["# Prepare inputs\n","inputs = processor_fine_tune(images=image, return_tensors=\"pt\").to(model_fine_tune.device)"]},{"cell_type":"code","execution_count":null,"id":"7a33d090","metadata":{"id":"7a33d090"},"outputs":[],"source":["# Generate caption\n","out = model_fine_tune.generate(**inputs)\n","caption1 = processor_fine_tune.decode(out[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"id":"e887f00a","metadata":{"id":"e887f00a"},"outputs":[],"source":["print(\"Caption:\", caption1)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}